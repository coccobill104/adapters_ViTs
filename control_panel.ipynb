{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "367c4cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models import ViT_B_16_Weights\n",
    "import copy\n",
    "\n",
    "from torchvision.datasets import Flowers102\n",
    "from torchvision import transforms\n",
    "\n",
    "from adapters.LoRAs import *\n",
    "from adapters.VeRAs import *\n",
    "from adapters.IA3 import *\n",
    "from adapters.PEFTclass import *\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f78fe271",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = ViT_B_16_Weights.IMAGENET1K_V1\n",
    "model = torchvision.models.vit_b_16(weights=weights)\n",
    "\n",
    "preprocess = weights.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cde7ed06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderBlock(\n",
       "  (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (self_attention): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (mlp): MLPBlock(\n",
       "    (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Dropout(p=0.0, inplace=False)\n",
       "    (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (4): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9ad6011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 144\n"
     ]
    }
   ],
   "source": [
    "nb_mlps = 0\n",
    "nb_att = 0\n",
    "\n",
    "for layer in model.encoder.layers: \n",
    "    nb_mlps += 1\n",
    "    nb_att += layer.self_attention.num_heads\n",
    "\n",
    "print(nb_mlps, nb_att)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28a2ae45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 768 out 3072\n",
      "4\n",
      "in 3072 out 768\n",
      "4\n",
      "6144\n",
      "in 768 out 3072\n",
      "4\n",
      "in 3072 out 768\n",
      "4\n",
      "6144\n",
      "in 768 out 3072\n",
      "4\n",
      "in 3072 out 768\n",
      "4\n",
      "6144\n",
      "in 768 out 3072\n",
      "4\n",
      "in 3072 out 768\n",
      "4\n",
      "6144\n",
      "in 768 out 3072\n",
      "4\n",
      "in 3072 out 768\n",
      "4\n",
      "6144\n",
      "in 768 out 3072\n",
      "4\n",
      "in 3072 out 768\n",
      "4\n",
      "6144\n",
      "in 768 out 3072\n",
      "4\n",
      "in 3072 out 768\n",
      "4\n",
      "6144\n",
      "in 768 out 3072\n",
      "4\n",
      "in 3072 out 768\n",
      "4\n",
      "6144\n",
      "in 768 out 3072\n",
      "4\n",
      "in 3072 out 768\n",
      "4\n",
      "6144\n",
      "in 768 out 3072\n",
      "4\n",
      "in 3072 out 768\n",
      "4\n",
      "6144\n",
      "in 768 out 3072\n",
      "4\n",
      "in 3072 out 768\n",
      "4\n",
      "6144\n",
      "in 768 out 3072\n",
      "4\n",
      "in 3072 out 768\n",
      "4\n",
      "6144\n"
     ]
    }
   ],
   "source": [
    "modified_model = PEFTViT(model, nb_classes=256, method='lora', attention=True, qkv = [False, True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49ecca47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PEFTViT(\n",
       "  (model): VisionTransformer(\n",
       "    (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (encoder): Encoder(\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (layers): Sequential(\n",
       "        (encoder_layer_0): EncoderBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): LoRASelfAttention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): LoRALinear(\n",
       "              (original_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): LoRALinear(\n",
       "              (original_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_1): EncoderBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): LoRASelfAttention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): LoRALinear(\n",
       "              (original_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): LoRALinear(\n",
       "              (original_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_2): EncoderBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): LoRASelfAttention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): LoRALinear(\n",
       "              (original_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): LoRALinear(\n",
       "              (original_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_3): EncoderBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): LoRASelfAttention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): LoRALinear(\n",
       "              (original_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): LoRALinear(\n",
       "              (original_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_4): EncoderBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): LoRASelfAttention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): LoRALinear(\n",
       "              (original_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): LoRALinear(\n",
       "              (original_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_5): EncoderBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): LoRASelfAttention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): LoRALinear(\n",
       "              (original_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): LoRALinear(\n",
       "              (original_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_6): EncoderBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): LoRASelfAttention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): LoRALinear(\n",
       "              (original_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): LoRALinear(\n",
       "              (original_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_7): EncoderBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): LoRASelfAttention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): LoRALinear(\n",
       "              (original_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): LoRALinear(\n",
       "              (original_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_8): EncoderBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): LoRASelfAttention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): LoRALinear(\n",
       "              (original_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): LoRALinear(\n",
       "              (original_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_9): EncoderBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): LoRASelfAttention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): LoRALinear(\n",
       "              (original_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): LoRALinear(\n",
       "              (original_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_10): EncoderBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): LoRASelfAttention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): LoRALinear(\n",
       "              (original_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): LoRALinear(\n",
       "              (original_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_11): EncoderBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): LoRASelfAttention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): LoRALinear(\n",
       "              (original_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): LoRALinear(\n",
       "              (original_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (heads): Linear(in_features=768, out_features=256, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "845c9ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=3072, out_features=768, bias=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlplayer = model.encoder.layers[0].self_attention\n",
    "model.encoder.layers[0].mlp[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f74842ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "368700"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "369409-709"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b399562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221184"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = 4\n",
    "12*2*r*768*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05bb0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. SETUP: Loading Base Model ---\n",
      "\n",
      "--- 2. WRAP: Creating PEFTViT ---\n",
      "\n",
      "--- 3. FREEZE: Setting Gradients ---\n",
      "[Info] Trainable: 65,377 | Total: 85,937,761 | %: 0.08%\n",
      "\n",
      "--- 4. MOCK TRAINING (Modifying Weights) ---\n",
      "   Modified lora_A[0,0]: 1.0000 -> 2.0000\n",
      "\n",
      "--- 5. SAVE: Testing state_dict override ---\n",
      "   Saved checkpoint size: 287.57 KB\n",
      "\n",
      "--- 6. LOAD: Testing load_state_dict override ---\n",
      "in 768 out 3072\n",
      "4\n",
      "in 3072 out 768\n",
      "4\n",
      "in 768 out 3072\n",
      "4\n",
      "in 3072 out 768\n",
      "4\n",
      "in 768 out 3072\n",
      "4\n",
      "in 3072 out 768\n",
      "4\n",
      "in 768 out 3072\n",
      "4\n",
      "in 3072 out 768\n",
      "4\n",
      "in 768 out 3072\n",
      "4\n",
      "in 3072 out 768\n",
      "4\n",
      "in 768 out 3072\n",
      "4\n",
      "in 3072 out 768\n",
      "4\n",
      "in 768 out 3072\n",
      "4\n",
      "in 3072 out 768\n",
      "4\n",
      "in 768 out 3072\n",
      "4\n",
      "in 3072 out 768\n",
      "4\n",
      "in 768 out 3072\n",
      "4\n",
      "in 3072 out 768\n",
      "4\n",
      "in 768 out 3072\n",
      "4\n",
      "in 3072 out 768\n",
      "4\n",
      "in 768 out 3072\n",
      "4\n",
      "in 3072 out 768\n",
      "4\n",
      "in 768 out 3072\n",
      "4\n",
      "in 3072 out 768\n",
      "4\n",
      "   Value before load: -0.0059\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for VisionTransformer:\n\tsize mismatch for heads.weight: copying a param with shape torch.Size([1, 768]) from checkpoint, the shape in current model is torch.Size([10, 768]).\n\tsize mismatch for heads.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([10]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 73\u001b[39m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(save_path):\n\u001b[32m     71\u001b[39m         os.remove(save_path)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43mcheck_PEFTViT\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mcheck_PEFTViT\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m#print(f\"   Value before load: {new_model.model.encoder.layers[0].mlp[0].vera_middle[ 0].item():.4f}\")\u001b[39;00m\n\u001b[32m     53\u001b[39m \n\u001b[32m     54\u001b[39m \n\u001b[32m     55\u001b[39m \n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Load the saved adapter\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# This calls our custom load_state_dict() method!\u001b[39;00m\n\u001b[32m     58\u001b[39m saved_weights = torch.load(save_path)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[43mnew_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msaved_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Value after load:  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_model.model.encoder.layers[\u001b[32m0\u001b[39m].mlp[\u001b[32m0\u001b[39m].lora_A[\u001b[32m0\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[32m0\u001b[39m].item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m#print(f\"   Value after load:  {new_model.model.encoder.layers[0].mlp[0].vera_middle[0].item():.4f}\")\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MVA/DeepLearningMVA/adapters_ViTs/adapters/PEFTclass.py:80\u001b[39m, in \u001b[36mPEFTViT.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[33;03mOVERRIDE: Forces strict=False to ignore missing base model weights.\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# We force strict=False so it doesn't complain about missing base weights\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m missing, unexpected = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unexpected) > \u001b[32m0\u001b[39m:\n\u001b[32m     83\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Unexpected keys found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munexpected\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MVA/DeepLearningMVA/adapters_ViTs/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2629\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2621\u001b[39m         error_msgs.insert(\n\u001b[32m   2622\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2623\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2624\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2625\u001b[39m             ),\n\u001b[32m   2626\u001b[39m         )\n\u001b[32m   2628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2629\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2630\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2631\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2632\u001b[39m         )\n\u001b[32m   2633\u001b[39m     )\n\u001b[32m   2634\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for VisionTransformer:\n\tsize mismatch for heads.weight: copying a param with shape torch.Size([1, 768]) from checkpoint, the shape in current model is torch.Size([10, 768]).\n\tsize mismatch for heads.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([10])."
     ]
    }
   ],
   "source": [
    "def check_PEFTViT():\n",
    "    print(\"--- 1. SETUP: Loading Base Model ---\")\n",
    "    base_model = torchvision.models.vit_b_16(weights=weights)\n",
    "\n",
    "    \n",
    "    print(\"\\n--- 2. WRAP: Creating PEFTViT ---\")\n",
    "    # Wrap the model\n",
    "    model = PEFTViT(base_model, nb_classes = 1, method='vera', r=4, attention= True, qkv = [False, True, True])\n",
    "\n",
    "    \n",
    "    print(\"\\n--- 3. FREEZE: Setting Gradients ---\")\n",
    "    # Simulate the freezing logic: Freeze everything EXCEPT head and LoRA\n",
    "    model.set_trainable_parameters()\n",
    "            \n",
    "    model.print_trainable_parameters()\n",
    "    # Expected: Small % (only head + lora params)\n",
    "\n",
    "    print(\"\\n--- 4. MOCK TRAINING (Modifying Weights) ---\")\n",
    "    # We manually modify a weight to prove saving works\n",
    "    with torch.no_grad():\n",
    "        # Modify one of the LoRA parameters\n",
    "\n",
    "\n",
    "        original_val = model.model.encoder.layers[0].mlp[0].vera_middle[0].item()\n",
    "        model.model.encoder.layers[0].mlp[0].vera_middle[0] += 1.0\n",
    "        new_val = model.model.encoder.layers[0].mlp[0].vera_middle[0].item()\n",
    "\n",
    "    print(f\"   Modified lora_A[0,0]: {original_val:.4f} -> {new_val:.4f}\")\n",
    "\n",
    "    print(\"\\n--- 5. SAVE: Testing state_dict override ---\")\n",
    "    save_path = \"temp_lora_checkpoint.pt\"\n",
    "    #save_path = \"temp_vera_checkpoint.pt\"\n",
    "\n",
    "    # This calls our custom state_dict() method!\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    file_size = os.path.getsize(save_path) / 1024\n",
    "    print(f\"   Saved checkpoint size: {file_size:.2f} KB\")\n",
    "    # If this were the full model, it would be ~45MB. \n",
    "    # Since it is <100KB, we know it saved ONLY the adapter.\n",
    "\n",
    "    print(\"\\n--- 6. LOAD: Testing load_state_dict override ---\")\n",
    "    # Create a fresh model (original weights) to prove we are loading the changes\n",
    "    fresh_base = torchvision.models.vit_b_16(weights=weights)\n",
    "    fresh_base.head = nn.Linear(512, 100)\n",
    "    \n",
    "    # Wrap it\n",
    "    new_model = PEFTViT(fresh_base, nb_classes=10, method='lora', r=4)\n",
    "\n",
    "    # Verify it has the OLD value before loading\n",
    "    print(f\"   Value before load: {new_model.model.encoder.layers[0].mlp[0].lora_A[0, 0].item():.4f}\")\n",
    "    #print(f\"   Value before load: {new_model.model.encoder.layers[0].mlp[0].vera_middle[ 0].item():.4f}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    # Load the saved adapter\n",
    "    # This calls our custom load_state_dict() method!\n",
    "    saved_weights = torch.load(save_path)\n",
    "    new_model.load_state_dict(saved_weights)\n",
    "    \n",
    "    print(f\"   Value after load:  {new_model.model.encoder.layers[0].mlp[0].lora_A[0, 0].item():.4f}\")\n",
    "    #print(f\"   Value after load:  {new_model.model.encoder.layers[0].mlp[0].vera_middle[0].item():.4f}\")\n",
    "    \n",
    "    if abs(new_model.model.encoder.layers[0].mlp[0].lora_A[0, 0].item() - new_val) < 1e-5:\n",
    "        print(\"\\n✅ SUCCESS: Weights restored correctly.\")\n",
    "    else:\n",
    "        print(\"\\n❌ FAILURE: Weights do not match.\")\n",
    "        \n",
    "    # Cleanup\n",
    "    if os.path.exists(save_path):\n",
    "        os.remove(save_path)\n",
    "\n",
    "check_PEFTViT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b78fd53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. SETUP: Loading Base Model ---\n",
      "\n",
      "--- 2. WRAP: Creating PEFTViT ---\n",
      "\n",
      "--- 3. FREEZE: Setting Gradients ---\n",
      "[Info] Trainable: 65,329 | Total: 85,900,849 | %: 0.08%\n",
      "\n",
      "--- 4. MOCK TRAINING (Modifying Weights) ---\n",
      "   Modified lora_A[0,0]: 1.0000 -> 2.0000\n",
      "\n",
      "--- 5. SAVE: Testing state_dict override ---\n",
      "   Saved checkpoint size: 287.57 KB\n",
      "\n",
      "--- 6. LOAD: Testing load_state_dict override ---\n",
      "in 768 out 3072\n",
      "4\n",
      "in 3072 out 768\n",
      "4\n",
      "in 768 out 3072\n",
      "4\n",
      "in 3072 out 768\n",
      "4\n",
      "in 768 out 3072\n",
      "4\n",
      "in 3072 out 768\n",
      "4\n",
      "in 768 out 3072\n",
      "4\n",
      "in 3072 out 768\n",
      "4\n",
      "in 768 out 3072\n",
      "4\n",
      "in 3072 out 768\n",
      "4\n",
      "in 768 out 3072\n",
      "4\n",
      "in 3072 out 768\n",
      "4\n",
      "in 768 out 3072\n",
      "4\n",
      "in 3072 out 768\n",
      "4\n",
      "in 768 out 3072\n",
      "4\n",
      "in 3072 out 768\n",
      "4\n",
      "in 768 out 3072\n",
      "4\n",
      "in 3072 out 768\n",
      "4\n",
      "in 768 out 3072\n",
      "4\n",
      "in 3072 out 768\n",
      "4\n",
      "in 768 out 3072\n",
      "4\n",
      "in 3072 out 768\n",
      "4\n",
      "in 768 out 3072\n",
      "4\n",
      "in 3072 out 768\n",
      "4\n",
      "   Value before load: -0.0000\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for VisionTransformer:\n\tsize mismatch for heads.weight: copying a param with shape torch.Size([1, 768]) from checkpoint, the shape in current model is torch.Size([10, 768]).\n\tsize mismatch for heads.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([10]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcheck_PEFTViT\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mcheck_PEFTViT\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m#print(f\"   Value before load: {new_model.model.encoder.layers[0].mlp[0].vera_middle[ 0].item():.4f}\")\u001b[39;00m\n\u001b[32m     53\u001b[39m \n\u001b[32m     54\u001b[39m \n\u001b[32m     55\u001b[39m \n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Load the saved adapter\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# This calls our custom load_state_dict() method!\u001b[39;00m\n\u001b[32m     58\u001b[39m saved_weights = torch.load(save_path)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[43mnew_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msaved_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Value after load:  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_model.model.encoder.layers[\u001b[32m0\u001b[39m].mlp[\u001b[32m0\u001b[39m].lora_A[\u001b[32m0\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[32m0\u001b[39m].item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m#print(f\"   Value after load:  {new_model.model.encoder.layers[0].mlp[0].vera_middle[0].item():.4f}\")\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MVA/DeepLearningMVA/adapters_ViTs/adapters/PEFTclass.py:80\u001b[39m, in \u001b[36mPEFTViT.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[33;03mOVERRIDE: Forces strict=False to ignore missing base model weights.\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# We force strict=False so it doesn't complain about missing base weights\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m missing, unexpected = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unexpected) > \u001b[32m0\u001b[39m:\n\u001b[32m     83\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Unexpected keys found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munexpected\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MVA/DeepLearningMVA/adapters_ViTs/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2629\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2621\u001b[39m         error_msgs.insert(\n\u001b[32m   2622\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2623\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2624\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2625\u001b[39m             ),\n\u001b[32m   2626\u001b[39m         )\n\u001b[32m   2628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2629\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2630\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2631\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2632\u001b[39m         )\n\u001b[32m   2633\u001b[39m     )\n\u001b[32m   2634\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for VisionTransformer:\n\tsize mismatch for heads.weight: copying a param with shape torch.Size([1, 768]) from checkpoint, the shape in current model is torch.Size([10, 768]).\n\tsize mismatch for heads.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([10])."
     ]
    }
   ],
   "source": [
    "check_PEFTViT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee64172e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
