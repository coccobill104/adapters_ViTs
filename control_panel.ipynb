{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367c4cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models import ViT_B_16_Weights\n",
    "import copy\n",
    "\n",
    "from torchvision.datasets import Flowers102\n",
    "from torchvision import transforms\n",
    "\n",
    "from LoRAs import *\n",
    "from VeRAs import *\n",
    "from IA3 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f78fe271",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = ViT_B_16_Weights.IMAGENET1K_V1\n",
    "model = torchvision.models.vit_b_16(weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "845c9ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlplayer = model.encoder.layers[0].self_attention\n",
    "mlplayer.embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36df6a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests():\n",
    "    print(\"--- Starting LoRA Implementation Tests ---\")\n",
    "    \n",
    "    # Load Model (dummy weights are fine, but we use ImageNet for realism)\n",
    "    print(\"1. Loading ViT-B/16...\")\n",
    "    model = torchvision.models.vit_b_16(weights=torchvision.models.ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "    \n",
    "    # Isolate one attention layer\n",
    "    original_layer = model.encoder.layers[0].self_attention\n",
    "    \n",
    "    # Create dummy input (Batch=1, Seq=197, Dim=768)\n",
    "    # 197 = 1 class token + (14x14) patches\n",
    "    dummy_input = torch.randn(1, 197, 768)\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # Test A: Does the new layer produce the exact same output?\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"\\nTest A: Output Consistency (Original vs LoRA with B=0)\")\n",
    "    \n",
    "    # Init LoRA layer\n",
    "    lora_layer = LoRASelfAttention(original_layer)\n",
    "\n",
    "    matrices = { 'A_q': torch.randn(( 4, 768)), 'B_q': torch.randn((768, 4))}\n",
    "    vera_layer = VeRASelfAttention(original_layer, matrices=matrices)\n",
    "\n",
    "    print('lora qkv', lora_layer.which_proj)\n",
    "    print('vera qkv', vera_layer.which_proj)\n",
    "    \n",
    "    # Set to eval mode to disable dropout in both\n",
    "    original_layer.eval()\n",
    "    lora_layer.eval()\n",
    "    vera_layer.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Original Forward (Pass x, x, x manually or rely on its forward)\n",
    "        # torchvision ViT passes x, x, x internaly\n",
    "        out_orig, _ = original_layer(dummy_input, dummy_input, dummy_input, need_weights=False)\n",
    "        \n",
    "        # LoRA Forward\n",
    "        out_lora, _ = lora_layer(dummy_input, dummy_input, dummy_input)\n",
    "        out_vera, _ = vera_layer(dummy_input, dummy_input, dummy_input)\n",
    "        \n",
    "    # Check difference\n",
    "    diff = torch.abs(out_orig - out_lora).max().item()\n",
    "    diff_vera = torch.abs(out_orig - out_vera).max().item()\n",
    "    print(f\"   Max absolute difference lora: {diff:.8f}\")\n",
    "    print(f\"   Max absolute difference vera: {diff_vera:.8f}\")\n",
    "    \n",
    "    if diff < 1e-5:\n",
    "        print(\"   ✅ LORA SUCCESS: Outputs match closely.\")\n",
    "    else:\n",
    "        print(\"   ❌ LORA FAILURE: Outputs diverge too much.\")\n",
    "\n",
    "    if diff_vera < 1e-5:\n",
    "        print(\"   ✅ VERA SUCCESS: Outputs match closely.\")\n",
    "    else:\n",
    "        print(\"   ❌ VERA FAILURE: Outputs diverge too much.\")\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Test B: Does LoRA actually modify the output when trained?\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"\\nTest B: LoRA Activation Check\")\n",
    "    \n",
    "    # Manually set LoRA B matrix to something non-zero to simulate training\n",
    "    with torch.no_grad():\n",
    "        lora_layer.lora_B_q.fill_(1.0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out_lora_active, _ = lora_layer(dummy_input, dummy_input, dummy_input)\n",
    "        \n",
    "    diff_active = torch.abs(out_orig - out_lora_active).max().item()\n",
    "    print(f\"   Diff after activating LoRA: {diff_active:.4f}\")\n",
    "    \n",
    "    if diff_active > 1e-3:\n",
    "        print(\"   ✅ SUCCESS: LoRA is modifying the output.\")\n",
    "    else:\n",
    "        print(\"   ❌ FAILURE: LoRA parameters are not affecting the output.\")\n",
    "\n",
    "    print(\"\\nTest B: vera Activation Check\")\n",
    "    \n",
    "\n",
    "\n",
    "    # Manually set VeRA b vector to something non-zero to simulate training\n",
    "    with torch.no_grad():\n",
    "        vera_layer.vera_b_q.fill_(1.0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out_vera_active, _ = vera_layer(dummy_input, dummy_input, dummy_input)\n",
    "        \n",
    "    diff_active_vera = torch.abs(out_orig - out_vera_active).max().item()\n",
    "    print(f\"   Diff after activating LoRA: {diff_active_vera:.4f}\")\n",
    "    \n",
    "    if diff_active_vera > 1e-3:\n",
    "        print(\"   ✅ SUCCESS: VeRA is modifying the output.\")\n",
    "    else:\n",
    "        print(\"   ❌ FAILURE: VeRA parameters are not affecting the output.\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Test C: Full Model Integration (Swapping the layer)\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"\\nTest C: Full Model Integration\")\n",
    "    \n",
    "    # Swap the layer in the model\n",
    "    model.encoder.layers[0].self_attention = lora_layer\n",
    "    \n",
    "    # Run full model forward pass\n",
    "    try:\n",
    "        logits = model(torch.randn(1, 3, 224, 224)) # Standard ImageNet input\n",
    "        print(f\"   Output shape: {logits.shape}\")\n",
    "        print(\"   ✅ SUCCESS: Full model forward pass complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ FAILURE: Model crashed with error: {e}\")\n",
    "\n",
    "\n",
    "    # Swap the layer in the model\n",
    "    model.encoder.layers[0].self_attention = vera_layer\n",
    "    \n",
    "    # Run full model forward pass\n",
    "    try:\n",
    "        logits = model(torch.randn(1, 3, 224, 224)) # Standard ImageNet input\n",
    "        print(f\"   Output shape: {logits.shape}\")\n",
    "        print(\"   ✅ SUCCESS: Full model forward pass complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ FAILURE: Model crashed with error: {e}\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Test D: Parameter Freeze Check\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"\\nTest D: Parameter Counting\")\n",
    "    \n",
    "    model.encoder.layers[0].self_attention = lora_layer\n",
    "\n",
    "    # Freeze base model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    # Unfreeze only LoRA params\n",
    "    lora_params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"lora_\" in name:\n",
    "            param.requires_grad = True\n",
    "            lora_params += param.numel()\n",
    "            \n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"   Total trainable parameters: {trainable_params}\")\n",
    "    \n",
    "    if trainable_params == lora_params and lora_params > 0:\n",
    "        print(\"   ✅ SUCCESS: Only LoRA parameters are trainable.\")\n",
    "    else:\n",
    "        print(f\"   ❌ FAILURE: Trainable params ({trainable_params}) != LoRA params ({lora_params})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    model.encoder.layers[0].self_attention = vera_layer\n",
    "    \n",
    "    # Freeze base model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    # Unfreeze only LoRA params\n",
    "    vera_params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"vera_\" in name:\n",
    "            param.requires_grad = True\n",
    "            vera_params += param.numel()\n",
    "            \n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"   Total trainable parameters: {trainable_params}\")\n",
    "    \n",
    "    if trainable_params == vera_params and vera_params > 0:\n",
    "        print(\"   ✅ SUCCESS: Only VeRA parameters are trainable.\")\n",
    "    else:\n",
    "        print(f\"   ❌ FAILURE: Trainable params ({trainable_params}) != VeRA params ({vera_params})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a970d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting LoRA Implementation Tests ---\n",
      "1. Loading ViT-B/16...\n",
      "\n",
      "Test A: Output Consistency (Original vs LoRA with B=0)\n",
      "lora qkv [True, False, False]\n",
      "vera qkv [True, False, False]\n",
      "   Max absolute difference lora: 0.00000222\n",
      "   Max absolute difference vera: 0.00000222\n",
      "   ✅ LORA SUCCESS: Outputs match closely.\n",
      "   ✅ VERA SUCCESS: Outputs match closely.\n",
      "\n",
      "Test B: LoRA Activation Check\n",
      "   Diff after activating LoRA: 1.2421\n",
      "   ✅ SUCCESS: LoRA is modifying the output.\n",
      "\n",
      "Test B: vera Activation Check\n",
      "   Diff after activating LoRA: 4.1646\n",
      "   ✅ SUCCESS: VeRA is modifying the output.\n",
      "\n",
      "Test C: Full Model Integration\n",
      "   Output shape: torch.Size([1, 1000])\n",
      "   ✅ SUCCESS: Full model forward pass complete.\n",
      "   Output shape: torch.Size([1, 1000])\n",
      "   ✅ SUCCESS: Full model forward pass complete.\n",
      "\n",
      "Test D: Parameter Counting\n",
      "   Total trainable parameters: 6144\n",
      "   ✅ SUCCESS: Only LoRA parameters are trainable.\n",
      "   Total trainable parameters: 772\n",
      "   ✅ SUCCESS: Only VeRA parameters are trainable.\n"
     ]
    }
   ],
   "source": [
    "run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0616b190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPBlock(\n",
      "  (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Dropout(p=0.0, inplace=False)\n",
      "  (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (4): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MLPBlock(\n",
      "  (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Dropout(p=0.0, inplace=False)\n",
      "  (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (4): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MLPBlock(\n",
      "  (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Dropout(p=0.0, inplace=False)\n",
      "  (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (4): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MLPBlock(\n",
      "  (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Dropout(p=0.0, inplace=False)\n",
      "  (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (4): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MLPBlock(\n",
      "  (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Dropout(p=0.0, inplace=False)\n",
      "  (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (4): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MLPBlock(\n",
      "  (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Dropout(p=0.0, inplace=False)\n",
      "  (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (4): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MLPBlock(\n",
      "  (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Dropout(p=0.0, inplace=False)\n",
      "  (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (4): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MLPBlock(\n",
      "  (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Dropout(p=0.0, inplace=False)\n",
      "  (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (4): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MLPBlock(\n",
      "  (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Dropout(p=0.0, inplace=False)\n",
      "  (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (4): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MLPBlock(\n",
      "  (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Dropout(p=0.0, inplace=False)\n",
      "  (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (4): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MLPBlock(\n",
      "  (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Dropout(p=0.0, inplace=False)\n",
      "  (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (4): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "MLPBlock(\n",
      "  (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Dropout(p=0.0, inplace=False)\n",
      "  (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (4): Dropout(p=0.0, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "layers = model.encoder.layers\n",
    "\n",
    "\n",
    "def apply_LoRA(model, r=4, mlps = True, mlpsblock=False, attention=False, qkv=[False, False, False]):\n",
    "    new_model = model.copy()\n",
    "    layers = new_model.encoder.layers\n",
    "    for layer in layers:\n",
    "        if mlps:\n",
    "            layer.mlp[0] = LoRALinear(layer.mlp[0], r=r)\n",
    "            layer.mlp[3] = LoRALinear(layer.mlp[3], r=r)\n",
    "        if mlpsblock:\n",
    "            layer.mlp = LoRALinear(layer.mlp, r=r)\n",
    "        if attention:\n",
    "            layer.self_attention = LoRASelfAttention(layer.self_attention, rank=r, q=qkv[0], k=qkv[1], v=qkv[2])\n",
    "    return new_model\n",
    "\n",
    "\n",
    "def apply_VeRA(model, r=4, mlps = True, mlpsblock=False, attention=False, qkv=[False, False, False], seed=0):\n",
    "\n",
    "    new_model = model.copy()\n",
    "    layers = new_model.encoder.layers\n",
    "    \n",
    "    if mlps:\n",
    "        mlp_dim = layers.mlp[0].out_features\n",
    "        A_mlp = torch.randn(r, mlp_dim)\n",
    "        B_mlp = torch.randn(mlp_dim, r)\n",
    "\n",
    "    if attention:\n",
    "        attention_dim = layers.self_attention.embed_dim\n",
    "        attention_matrices = {'A_q': torch.randn(r, attention_dim), 'A_k': torch.randn(r, attention_dim), 'A_v': torch.randn(r, attention_dim), \n",
    "                    'B_q': torch.randn(attention_dim, r), 'B_k':  torch.randn(attention_dim, r), 'B_v':  torch.randn(attention_dim, r)\n",
    "                    }\n",
    "        \n",
    "    for layer in layers: \n",
    "        if mlps:\n",
    "            layer.mlp[0] = VeRALinear(layer.mlp[0], A_mlp, B_mlp r=r)\n",
    "            layer.mlp[3] = VeRALinear(layer.mlp[3], A_mlp, B_mlp, r=r)\n",
    "\n",
    "        if attention:\n",
    "            layer.self_attention = VeRASelfAttention(layer.self_attention, rank=r, q=qkv[0], k=qkv[1], v=qkv[2], matrices = attention_matrices)\n",
    "\n",
    "    return new_model\n",
    "\n",
    "\n",
    "def apply_IA3(model, mlps=True, attention=True, qkv = [False, True, True]):\n",
    "\n",
    "    new_model = model.copy()\n",
    "    layers = new_model.layers\n",
    "\n",
    "    for layer in layers: \n",
    "        if mlps:\n",
    "            layer.mlp[3] = IA3Linear(layer)\n",
    "        \n",
    "        if attention:\n",
    "            layer.self_attention = IA3SelfAttention(layer.self.attention)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
